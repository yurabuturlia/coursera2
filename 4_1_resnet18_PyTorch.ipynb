{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yurabuturlia/coursera2/blob/main/4_1_resnet18_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoH7c6uyVDr3"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC3348UiVDr8"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNTquDtVDr8"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NqHTlwwVDr9"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpBJ_MNHVDr-"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEKU2oAVDr-"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zS-YeWiVDr_"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i00kYW0VDsA"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZcnCyy1VDsB",
        "outputId": "c269c288-d724-44c7-dedf-1c16333fdb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-22 19:58:23--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip.1’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  32.5MB/s    in 81s     \n",
            "\n",
            "2022-11-22 19:59:44 (30.6 MB/s) - ‘Positive_tensors.zip.1’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZR9ecVpVDsC",
        "outputId": "91bd4eeb-9a1c-47b9-e829-0975dfb54cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Positive_tensors/5114.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKjl-IZMVDsD",
        "outputId": "bfedea6c-d5d3-4b1f-8fcf-3cb2470ccb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-22 20:43:06--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip.1’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  35.4MB/s    in 62s     \n",
            "\n",
            "2022-11-22 20:44:09 (32.3 MB/s) - ‘Negative_tensors.zip.1’ saved [2111408108/2111408108]\n",
            "\n",
            "replace Negative_tensors/5114.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_kRwLaNVDsE"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28xDO6MVDsE",
        "outputId": "312fcd91-f159-4ca9-91a0-e4dc216e196c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uviucoLKVDsF"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY5npqmRVDsF"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqKniMGXVDsF",
        "outputId": "923d893a-2dde-46c7-961b-5fdd72403616"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdba81341d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Au6wyeQBVDsG"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW2-_pTbVDsG"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyAGIGxfVDsG"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKf90tvqVDsH"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buPZk_sHVDsH",
        "outputId": "279b596e-3780-4b43-8ba4-6f2f64875ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7VR0mQVDsI"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e88sIEoaVDsI",
        "outputId": "cddc3ac6-0310-4f2f-bb81-efe1906214e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQAkfbQ5VDsI"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw6SnbZzVDsJ"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUSX_2oVDsJ"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6hQa8tHVDsJ",
        "outputId": "80f77ef1-23da-4edf-95a0-1e75dac1570b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "\n",
        "# Type your code here\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnQE6PO7VDsJ"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VGyMUz2yVDsJ"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "\n",
        "\n",
        "# Type your code here\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI1HlNiMVDsK"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t83x-1kTVDsK"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pdSlcwgpVDsK"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9ll2wuVDsK"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWPCzUuxVDsK",
        "outputId": "85f45274-b96a-412a-af16-22a144c13b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF5XS3HeVDsL"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cCx1fuvVDsL"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCTIq-hOVDsL"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EDhEhjBCVDsL"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "\n",
        "# Type your code here\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUbT9u64VDsL"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LFZztwnkVDsM"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m38t_sd_VDsM"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wh_0ZdCSVDsM"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po8wSUIIVDsM"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq9c82LZVDsM"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qQ1At8WMVDsN"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "        # calculate loss \n",
        "        loss = criterion(z,y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data,1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct+=(yhat==y_test).sum().item()\n",
        "   \n",
        "    accuracy=correct/N_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x7HnlQ7VDsN"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s00nke5qVDsN",
        "outputId": "65e96947-5477-41ad-d5c4-4fe577f3b4b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9942"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "z-DZSGsXVDsO",
        "outputId": "6667324b-3ef1-4a01-edf7-94fc92429d5c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH9ZRcGYVDsO"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0mETe8VDsO"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3uIqhXA-VDsO"
      },
      "outputs": [],
      "source": [
        "val_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=1000)\n",
        "N_test=len(validation_dataset)\n",
        "for x_test, y_test in val_loader:\n",
        "        model.eval()\n",
        "        z = model(x_test)\n",
        "        _, yhat = torch.max(z.data,1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(y_test)):\n",
        "        if yhat[i]!=y_test[i]:\n",
        "            print('sample ',i,' predicted value: ',yhat[i].item(),' actual value: ',y_test[i].item())\n",
        "            count+=1\n",
        "            if count == 4:\n",
        "              break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIG9KKsr06is",
        "outputId": "60573fef-8a95-47d1-e0b3-40f0b4a8e5a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample  155  predicted value:  1  actual value:  0\n",
            "sample  312  predicted value:  0  actual value:  1\n",
            "sample  401  predicted value:  1  actual value:  0\n",
            "sample  623  predicted value:  1  actual value:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gSy6jgLVDsO"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhZz7WoyVDsP"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94SrxUkVDsP"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJgR1WXtVDsP"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}